{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CE49X – Lab 5: Bias–Variance Tradeoff using the Air Quality Dataset\n",
        "\n",
        "**Course:** CE49X – Introduction to Computational Thinking and Data Science for Civil Engineers\n",
        "\n",
        "**Instructor:** Dr. Eyuphan Koç\n",
        "\n",
        "**Semester:** Fall 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this lab, you will:\n",
        "- Understand the **bias–variance tradeoff** in machine learning.\n",
        "- Implement and compare **linear** and **polynomial regression** models.\n",
        "- Visualize **training** and **testing errors** as model complexity changes.\n",
        "- Interpret **underfitting** and **overfitting** phenomena using real environmental data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better-looking plots\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Note: Download AirQualityUCI.csv from https://archive.ics.uci.edu/dataset/360/air+quality\n",
        "# Place it in the same directory as this notebook or update the path below\n",
        "\n",
        "try:\n",
        "    # Read CSV with semicolon separator and comma decimal separator\n",
        "    df = pd.read_csv('AirQualityUCI.csv', sep=';', decimal=',')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset not found. Please download AirQualityUCI.csv from:\")\n",
        "    print(\"https://archive.ics.uci.edu/dataset/360/air+quality\")\n",
        "    print(\"\\nAlternatively, we can try to download it programmatically...\")\n",
        "    \n",
        "    # Try to download the dataset\n",
        "    import urllib.request\n",
        "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip'\n",
        "    print(f\"Attempting to download from: {url}\")\n",
        "    print(\"Please download manually if this fails.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values (-200 indicates missing data)\n",
        "print(\"Columns in dataset:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nChecking for missing values (represented as -200):\")\n",
        "\n",
        "# Clean column names (remove leading/trailing spaces)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Select the features and target\n",
        "features = ['T', 'RH', 'AH']\n",
        "target = 'CO(GT)'\n",
        "\n",
        "# Check if columns exist\n",
        "print(\"\\nAvailable columns:\")\n",
        "for col in df.columns:\n",
        "    if any(f in col for f in features) or target in col:\n",
        "        print(f\"  '{col}'\")\n",
        "\n",
        "# Select features and target\n",
        "selected_cols = features + [target]\n",
        "print(f\"\\nSelected columns: {selected_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the selected columns\n",
        "data = df[selected_cols].copy()\n",
        "\n",
        "# Replace -200 with NaN (missing values)\n",
        "data = data.replace(-200, np.nan)\n",
        "\n",
        "# Convert columns to numeric (handles any remaining string issues)\n",
        "for col in data.columns:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "print(f\"Data shape after removing missing values: {data.shape}\")\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(data.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = data[features].values\n",
        "y = data[target].values\n",
        "\n",
        "# Split into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fit Models of Increasing Complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lists to store errors\n",
        "degrees = list(range(1, 11))  # Polynomial degrees from 1 to 10\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "print(\"Training polynomial regression models...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly_features.fit_transform(X_train)\n",
        "    X_test_poly = poly_features.transform(X_test)\n",
        "    \n",
        "    # Train linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train_poly)\n",
        "    y_test_pred = model.predict(X_test_poly)\n",
        "    \n",
        "    # Calculate Mean Squared Error\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    \n",
        "    train_errors.append(train_mse)\n",
        "    test_errors.append(test_mse)\n",
        "    \n",
        "    print(f\"Degree {degree:2d}: Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"\\nModel training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Plot the Validation Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the optimal degree (minimum test error)\n",
        "optimal_degree_idx = np.argmin(test_errors)\n",
        "optimal_degree = degrees[optimal_degree_idx]\n",
        "optimal_test_error = test_errors[optimal_degree_idx]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(degrees, train_errors, 'o-', label='Training Error', linewidth=2, markersize=8)\n",
        "plt.plot(degrees, test_errors, 's-', label='Testing Error', linewidth=2, markersize=8)\n",
        "\n",
        "# Mark the optimal degree\n",
        "plt.axvline(x=optimal_degree, color='red', linestyle='--', alpha=0.7, \n",
        "            label=f'Optimal Degree ({optimal_degree})')\n",
        "plt.plot(optimal_degree, optimal_test_error, 'ro', markersize=12, \n",
        "         label=f'Minimum Test Error ({optimal_test_error:.4f})')\n",
        "\n",
        "# Add annotations for regions\n",
        "plt.axvspan(1, 3, alpha=0.1, color='blue', label='Underfitting Region')\n",
        "plt.axvspan(optimal_degree-1, optimal_degree+1, alpha=0.1, color='green', \n",
        "           label='Optimal Complexity')\n",
        "plt.axvspan(8, 10, alpha=0.1, color='red', label='Overfitting Region')\n",
        "\n",
        "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Mean Squared Error (MSE)', fontsize=12, fontweight='bold')\n",
        "plt.title('Bias–Variance Tradeoff: Training vs Testing Error', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(degrees)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOptimal polynomial degree: {optimal_degree}\")\n",
        "print(f\"Minimum test error: {optimal_test_error:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Discussion Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Which polynomial degree gives the best generalization?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Based on the validation curve above, the polynomial degree that gives the best generalization is the degree with the minimum test error (typically found in the middle range, e.g., degree 3-5). This degree achieves the lowest testing error, indicating the best balance between bias and variance.\n",
        "\n",
        "At this optimal degree:\n",
        "- The model captures the underlying patterns in the data without overfitting to noise\n",
        "- The gap between training and testing errors is minimized\n",
        "- The model generalizes well to unseen data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Describe how the training and testing errors change as degree increases.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "As the polynomial degree increases:\n",
        "\n",
        "1. **Training Error:** Generally decreases (or remains low) as the model becomes more complex. Higher-degree polynomials have more parameters and can fit the training data more closely, potentially achieving very low training errors.\n",
        "\n",
        "2. **Testing Error:** Initially decreases as the model complexity increases (reducing bias), reaches a minimum at the optimal degree, and then increases as the model becomes too complex (increasing variance/overfitting).\n",
        "\n",
        "3. **Gap Between Errors:** The gap between training and testing errors typically increases with higher degrees. A small gap indicates good generalization, while a large gap suggests overfitting.\n",
        "\n",
        "This pattern demonstrates the classic bias-variance tradeoff: simple models have high bias (underfitting), complex models have high variance (overfitting), and the optimal model balances both.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Explain how bias and variance manifest in this dataset.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bias (Underfitting):**\n",
        "- Manifested in low-degree polynomial models (degree 1-2)\n",
        "- These models are too simple to capture the complex relationships between temperature (T), relative humidity (RH), absolute humidity (AH), and CO concentration\n",
        "- Both training and testing errors are high, indicating the model is missing important patterns\n",
        "- The model makes systematic errors because it cannot represent the true underlying function\n",
        "\n",
        "**Variance (Overfitting):**\n",
        "- Manifested in high-degree polynomial models (degree 8-10)\n",
        "- These models are too complex and fit the noise in the training data rather than the signal\n",
        "- Training error is very low, but testing error increases significantly\n",
        "- The large gap between training and testing errors indicates poor generalization\n",
        "- The model memorizes training data patterns that don't generalize to new data\n",
        "\n",
        "**Optimal Balance:**\n",
        "- At the optimal degree, the model captures the true relationships without overfitting\n",
        "- Both bias and variance are minimized, resulting in the best generalization performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: How might sensor noise or missing data affect the bias–variance tradeoff?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Sensor Noise:**\n",
        "- **Increases variance:** Noisy data makes it harder for the model to distinguish signal from noise\n",
        "- **Encourages overfitting:** Complex models may try to fit the noise, leading to poor generalization\n",
        "- **Shifts optimal complexity:** The optimal degree may be lower when noise is present, as simpler models are more robust to noise\n",
        "- **Increases testing error:** Even at the optimal complexity, noise contributes to irreducible error\n",
        "\n",
        "**Missing Data:**\n",
        "- **Reduces effective sample size:** After removing missing values, we have fewer training examples\n",
        "- **Increases variance:** With less data, models are more prone to overfitting\n",
        "- **Affects feature relationships:** Missing data patterns (if not random) can introduce bias\n",
        "- **May require simpler models:** With less data, simpler models (lower degrees) may generalize better\n",
        "\n",
        "**In this dataset:**\n",
        "- The dataset uses -200 to indicate missing values, which we handled by removing those rows\n",
        "- Sensor measurements inherently contain noise from environmental factors and sensor limitations\n",
        "- These factors likely contribute to the observed bias-variance tradeoff and may explain why very high-degree polynomials don't perform well\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Cross-Validation Analysis (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Use cross-validation for more robust evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_train_errors = []\n",
        "cv_test_errors = []\n",
        "\n",
        "print(\"Performing 5-fold cross-validation...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_poly = poly_features.fit_transform(X)\n",
        "    \n",
        "    # Train model\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    # Cross-validation (negative MSE, so we negate to get positive MSE)\n",
        "    cv_scores = -cross_val_score(model, X_poly, y, cv=5, \n",
        "                                 scoring='neg_mean_squared_error')\n",
        "    \n",
        "    # Also compute on full training set for comparison\n",
        "    model.fit(X_poly, y)\n",
        "    train_pred = model.predict(X_poly)\n",
        "    train_mse = mean_squared_error(y, train_pred)\n",
        "    \n",
        "    cv_train_errors.append(train_mse)\n",
        "    cv_test_errors.append(cv_scores.mean())\n",
        "    \n",
        "    print(f\"Degree {degree:2d}: CV MSE = {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
        "\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross-validation results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(degrees, cv_train_errors, 'o-', label='Training Error (Full Dataset)', \n",
        "         linewidth=2, markersize=8)\n",
        "plt.plot(degrees, cv_test_errors, 's-', label='Cross-Validation Error (5-fold)', \n",
        "         linewidth=2, markersize=8)\n",
        "\n",
        "# Find optimal degree from CV\n",
        "optimal_cv_degree_idx = np.argmin(cv_test_errors)\n",
        "optimal_cv_degree = degrees[optimal_cv_degree_idx]\n",
        "optimal_cv_error = cv_test_errors[optimal_cv_degree_idx]\n",
        "\n",
        "plt.axvline(x=optimal_cv_degree, color='red', linestyle='--', alpha=0.7,\n",
        "            label=f'Optimal CV Degree ({optimal_cv_degree})')\n",
        "plt.plot(optimal_cv_degree, optimal_cv_error, 'ro', markersize=12)\n",
        "\n",
        "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Mean Squared Error (MSE)', fontsize=12, fontweight='bold')\n",
        "plt.title('Bias–Variance Tradeoff: Cross-Validation Analysis', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(degrees)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOptimal degree from cross-validation: {optimal_cv_degree}\")\n",
        "print(f\"Optimal degree from train/test split: {optimal_degree}\")\n",
        "print(f\"\\nCross-validation provides a more robust estimate of model performance.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This lab successfully demonstrated the bias-variance tradeoff using polynomial regression on the Air Quality dataset. Key findings:\n",
        "\n",
        "1. **Optimal Model Complexity:** Found the polynomial degree that best balances bias and variance\n",
        "2. **Underfitting vs Overfitting:** Observed how simple models underfit and complex models overfit\n",
        "3. **Generalization:** Demonstrated the importance of evaluating models on unseen test data\n",
        "4. **Real-world Application:** Applied machine learning concepts to environmental sensor data\n",
        "\n",
        "The results show that finding the right model complexity is crucial for building models that generalize well to new data.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
